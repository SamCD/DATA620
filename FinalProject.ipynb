{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite as bi\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import mplleaflet\n",
    "\n",
    "from collections import OrderedDict \n",
    "from operator import getitem \n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting full list of businesses reviewed\n",
    "busDF = pd.read_json(r\"C:\\Users\\scohendevries\\Documents\\DATA620\\yelp_dataset\\business.json\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting format of business table\n",
    "#busDF.head()\n",
    "busDF.groupby('state').count()['business_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large number of reviews, reading in chunks\n",
    "size = 1000000\n",
    "review = pd.read_json(r\"C:\\Users\\scohendevries\\Documents\\DATA620\\yelp_dataset\\review.json\", lines=True,\n",
    "                      dtype={'review_id':str,'user_id':str,\n",
    "                             'business_id':str,'stars':int,\n",
    "                             'date':str,'text':str,'useful':int,\n",
    "                             'funny':int,'cool':int},\n",
    "                      chunksize=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then iterating through chunks\n",
    "chunk_list = []\n",
    "for chunk_review in review:\n",
    "    #dropping fields out of scope for this study\n",
    "    chunk_review = chunk_review.drop(['review_id','useful','funny','cool'], axis=1)\n",
    "    chunk_review = chunk_review.rename(columns={'stars': 'review_stars'})\n",
    "    chunk_list.append(chunk_review)\n",
    "revDF = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting format of reviews table\n",
    "revDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following same process for Users data\n",
    "users = pd.read_json(r\"C:\\Users\\scohendevries\\Documents\\DATA620\\yelp_dataset\\user.json\", lines=True,\n",
    "                      dtype={'user_id':str,'name':str,\n",
    "                             'review_count':int,'yelping_since':str,\n",
    "                             'friends':list,'useful':int,'funny':int,\n",
    "                             'cool':int,'fans':int,'elite':list,'average_stars':float,\n",
    "                             'compliment_hot':int,'compliment_more':int,'compliment_profile':int,\n",
    "                             'compliment_cute':int,'compliment_list':int,'compliment_note':int,\n",
    "                             'compliment_plain':int,'compliment_cool':int,'compliment_funny':int,\n",
    "                             'compliment_writer':int,'compliment_photos':int\n",
    "                            },\n",
    "                      chunksize=size\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_list = []\n",
    "for chunk_review in users:\n",
    "    chunk_review = chunk_review.drop(['yelping_since'\n",
    "                                      ,'useful'\n",
    "                                      ,'funny'\n",
    "                                      ,'cool'\n",
    "                                      ,'compliment_hot'\n",
    "                                      ,'compliment_more'\n",
    "                                      ,'compliment_profile'\n",
    "                                      ,'compliment_cute'\n",
    "                                      ,'compliment_list'\n",
    "                                      ,'compliment_note'\n",
    "                                      ,'compliment_plain'\n",
    "                                      ,'compliment_cool'\n",
    "                                      ,'compliment_funny'\n",
    "                                      ,'compliment_writer'\n",
    "                                      ,'compliment_photos'\n",
    "                                     ], axis=1)\n",
    "    chunk_list.append(chunk_review)\n",
    "userDF = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processed numbers from data set\n",
    "print(str(len(busDF.index)) + ' businesses')\n",
    "print(str(len(userDF.index)) + ' users')\n",
    "print(str(len(revDF.index)) + ' reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping businesses by regions for greater numbers\n",
    "regionDF = pd.read_csv('https://raw.githubusercontent.com/cphalpert/census-regions/master/us%20census%20bureau%20regions%20and%20divisions.csv')\n",
    "busDF = pd.merge(busDF,regionDF,how='inner',left_on='state',right_on='State Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focus on single category\n",
    "catDF = busDF.assign(categories = busDF.categories\n",
    "                         .str.split(', ')).explode('categories')\n",
    "catDF.categories.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further filtering data set by looking at top 10% by number of reviews\n",
    "#here we are also grouping by region to ensure we are getting samples from all regions, rather than skewing numbers towards densely populated areas\n",
    "busDF = busDF[busDF['categories'].str.contains('Restaurants|Nightlife|Bars|Food',case=False,na=False)]\n",
    "busDF['q'] = busDF.groupby(['Division'])['review_count'].rank(pct=True)\n",
    "busDF = busDF[busDF['q'] > 0.9]\n",
    "userDF = userDF[userDF.review_count > userDF.review_count.quantile(0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in our filtered data set, we still see skewed numbers by state\n",
    "#this is a function of the data set\n",
    "print(str(len(busDF.index)) + ' businesses')\n",
    "print(str(len(userDF.index)) + ' users')\n",
    "\n",
    "busDF.groupby('state').count()['business_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(pd.merge(revDF,userDF,how='inner',on='user_id'),busDF,how='inner',on='business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\n",
    "    ['user_id'\n",
    "     ,'business_id'\n",
    "     ,'city'\n",
    "     ,'state'\n",
    "     ,'latitude'\n",
    "     ,'longitude'\n",
    "     ,'Region'\n",
    "     ,'Division'\n",
    "     ,'name_y'\n",
    "     ,'review_stars'\n",
    "     ,'stars'\n",
    "     ,'average_stars'\n",
    "     ,'categories'\n",
    "     ,'text'\n",
    "     ,'date']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are limiting our analysis to just under 1MM reviews\n",
    "print(str(len(df.index)) + ' reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename({'stars':'avgStars_biz'\n",
    "           ,'average_stars':'avgStars_usr'\n",
    "          }\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using nltk to filter out stopwords, numbers and punctuation\n",
    "def nonStops(text):\n",
    "    return [word for word in word_tokenize(text) \\\n",
    "            if word.lower() not in stopWords \\\n",
    "            and not word.isdigit()\\\n",
    "            and word.isalpha()\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byReg = {}\n",
    "allWords = {}\n",
    "for i in df.itertuples():\n",
    "    t = nonStops(i.text)\n",
    "    #leveling weights here, so that a review score of 3 is seen as neutral, and anything above/below is positive/negative\n",
    "    revWt = i.review_stars - 3\n",
    "    #creating a dictionary of all words to differentiate weighting by region\n",
    "    for w in t:\n",
    "        if w not in allWords.keys():\n",
    "            allWords[w] = {'n':1,'wt':revWt,'mean':revWt,'stDev':0}\n",
    "        else:\n",
    "            allWords[w]['n'] += 1\n",
    "            allWords[w]['wt'] += revWt\n",
    "            allWords[w]['mean'] = allWords[w]['wt']/allWords[w]['n']\n",
    "        if w not in byReg.keys():\n",
    "            byReg[w] = {}\n",
    "        if i.Division not in byReg[w].keys():\n",
    "            byReg[w][i.Division] = {'n':1,'wt':revWt,'mean':revWt}\n",
    "        else:\n",
    "            byReg[w][i.Division]['n'] += 1\n",
    "            byReg[w][i.Division]['wt'] += revWt\n",
    "            byReg[w][i.Division]['mean'] = byReg[w][i.Division]['wt']/byReg[w][i.Division]['n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in allWords.keys():\n",
    "    mn = allWords[i]['wt']/allWords[i]['n']\n",
    "    sdn = 0\n",
    "    n = 0\n",
    "    for j in byReg[i].keys():\n",
    "        sdn += byReg[i][j]['mean'] - mn\n",
    "        n += 1\n",
    "    sd = (sdn**2.0)/(n)\n",
    "    allWords[i]['stDev'] = sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to find words which might have their sentiment vary by region, we find those with the most volatility in terms of related review scores\n",
    "OrderedDict(sorted(allWords.items(),key = lambda x: getitem(x[1], 'stDev'),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at sample reviews for one of the more voltile words\n",
    "df[df['text'].str.contains('rental')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The second part of the analysis involves a network analysis of business, based on patrons\n",
    "usr = df['user_id'].values.tolist()\n",
    "biz = df['business_id'].values.tolist()\n",
    "\n",
    "g=nx.from_pandas_edgelist(df,'user_id','business_id',['city','state','latitude','longitude'])\n",
    "\n",
    "print(len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a bipartite graph with Businesses as the top node\n",
    "bNet = bi.weighted_projected_graph(g, biz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(nx.connected_components(bNet), key = len, reverse=True)[0:20]:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a set of functions to utilize the island method for network analysis\n",
    "def trim_edges(g, weight=1):\n",
    "    g2=nx.Graph()\n",
    "    for f, to, edata in g.edges(data=True):\n",
    "        if edata['weight'] > weight:\n",
    "            g2.add_edge(f,to,weight=edata['weight'])\n",
    "    return g2\n",
    "\n",
    "def island_method(g, iterations=5):\n",
    "    weights= [edata['weight'] for f,to,edata in g.edges(data=True)]\n",
    "    mn=int(min(weights))\n",
    "    mx=int(max(weights))\n",
    "    #compute the size of the step, so we get a reasonable step in iterations\n",
    "    step=int((mx-mn)/iterations)\n",
    "    return [[threshold, trim_edges(g, threshold)] for threshold in range(mn,mx,step)]\n",
    "\n",
    "def sorted_map(d):\n",
    "    ms = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bweights=[edata['weight'] for f,t,edata in bNet.edges(data=True)]\n",
    "nx.draw_networkx(bNet,width=bweights, node_size=10, with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnet_trim=trim_edges(bNet, weight=75)\n",
    "weights=[math.log(edata['weight']) for f,t,edata in bnet_trim.edges(data=True)]\n",
    "nx.draw_networkx(bnet_trim,width=weights, node_size=10, with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bPos = busDF[['business_id','latitude','longitude']].set_index('business_id')[['latitude', 'longitude']].T.apply(tuple)\n",
    "pos = bPos.to_dict()\n",
    "#pos\n",
    "#nx.draw_networkx(bNet,pos)\n",
    "nx.draw(bnet_trim, nx.get_node_attributes(bnet_trim, 'pos'), with_labels=True, node_size=0)\n",
    "#mplleaflet.display(fig=ax.figure) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top connected businesses could be interpreted as \"tourist\" centers, in that they represent places of business which are\n",
    "#reviews by patrons who review/patronize other businesses (presumably in other areas)\n",
    "btwn = nx.betweenness_centrality(bnet_trim)\n",
    "top_biz = []\n",
    "for i in sorted_map(btwn)[:20]:\n",
    "    top_biz.append(i[0])\n",
    "busDF[busDF['business_id'].isin(top_biz)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://towardsdatascience.com/converting-yelp-dataset-to-csv-using-pandas-2a4c8f03bd88\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
